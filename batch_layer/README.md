# Batch Layer - Market Analytics

This directory contains the **Batch Layer** implementation for analyzing correlations between Polymarket prediction markets and Binance cryptocurrency spot prices using the Lambda Architecture.

---

## ðŸ“ Directory Structure

```
batch_layer/
â”œâ”€â”€ hive/
â”‚   â”œâ”€â”€ create_tables.sql          # Hive DDL for external tables
â”‚   â””â”€â”€ repair_partitions.sql      # Partition discovery script
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ batch_analytics.py         # Main PySpark batch job
â”‚   â””â”€â”€ config.py                  # Configuration parameters
â”œâ”€â”€ hbase/
â”‚   â””â”€â”€ create_table.sh            # HBase table creation
â”œâ”€â”€ hadoop/
â”‚   â””â”€â”€ .gitkeep                   # HDFS directory conventions
â””â”€â”€ run_batch_analytics.sh         # Complete pipeline executor
```

---

## ðŸŽ¯ What This Does

The batch layer performs the following analysis:

1. **Reads historical data** from Hive tables (Binance + Polymarket)
2. **Aggregates data** into 15-minute windows
3. **Joins** prediction market probabilities with actual price movements
4. **Computes correlation metrics**:
   - Did high prediction probability result in price increase?
   - Prediction accuracy (correct bull/bear vs failed predictions)
5. **Stores results** in HBase for serving layer access

---

## ðŸš€ Quick Start

### Prerequisites

Ensure services are running:

```bash
# Start Hadoop, Hive, HBase
sudo /home/vagrant/scripts/bootstrap.sh

# Verify services
hdfs dfsadmin -report
hive -e "SHOW DATABASES;"
echo "status" | hbase shell -n
```

### Run the Complete Pipeline

```bash
cd ~/Crypto-Options-vs-Rates

# Make executable
chmod +x batch_layer/run_batch_analytics.sh
chmod +x batch_layer/hbase/create_table.sh

# Run full pipeline (all data)
batch_layer/run_batch_analytics.sh

# Or process specific date
batch_layer/run_batch_analytics.sh 2026-01-11
```

This will:
1. Create Hive tables
2. Discover partitions
3. Create HBase table
4. Run Spark analytics
5. Store results in HBase

---

## ðŸ“Š Data Flow

```
HDFS /user/vagrant/cleansed/
â”œâ”€â”€ binance/              (Parquet, partitioned by date)
â”‚   â””â”€â”€ date=2026-01-11/
â””â”€â”€ polymarket_trade/     (Parquet, partitioned by date)
    â””â”€â”€ date=2026-01-11/
          â†“
     Hive Tables
     (binance_klines, polymarket_orderbook)
          â†“
     PySpark Batch Job
     (15-min window aggregation + join)
          â†“
     HBase: market_analytics
     (price_data, bet_data, analysis column families)
```

---

## ðŸ”§ Step-by-Step Execution

If you prefer manual execution:

### 1. Create Hive Tables

```bash
hive -f batch_layer/hive/create_tables.sql
hive -f batch_layer/hive/repair_partitions.sql
```

Verify:
```sql
hive -e "SELECT COUNT(*) FROM binance_klines;"
hive -e "SELECT COUNT(*) FROM polymarket_orderbook;"
```

### 2. Create HBase Table

```bash
bash batch_layer/hbase/create_table.sh
```

Verify:
```bash
echo "describe 'market_analytics'" | hbase shell
```

### 3. Run Spark Job

```bash
spark-submit \
    --master local[*] \
    --driver-memory 2g \
    --py-files batch_layer/spark/config.py \
    batch_layer/spark/batch_analytics.py
```

---

## ðŸ“ˆ Analysis Logic

### Binance Aggregation (15-minute OHLC)

- **Window**: 15 minutes
- **Metrics**: Open, High, Low, Close, Volume
- **Example**: 12:00-12:15 â†’ single aggregated candle

### Polymarket Aggregation (15-minute Probability)

- **Window**: 15 minutes
- **Metrics**: Average probability, Max probability, Activity count
- **Example**: 12:00-12:15 â†’ avg(price) where price is "Up" probability

### Join & Correlation

```python
# Price movement calculation
price_movement = (close - open) / open

# Prediction evaluation
if avg_probability > 0.60 and price_movement > 0:
    result = "CORRECT_BULL"  # Market predicted UP, price went UP
elif avg_probability > 0.60 and price_movement < 0:
    result = "FAILED_BULL"   # Market predicted UP, price went DOWN
...
```

---

## ðŸ—„ï¸ HBase Schema

**Table**: `market_analytics`

**Row Key**: `{SYMBOL}_{REVERSE_TIMESTAMP}`
- Example: `BTCUSDT_9999999998232799600`
- Reverse timestamp enables newest-first scans

**Column Families**:

| Family | Columns | Description |
|--------|---------|-------------|
| `price_data` | open, close, high, low, volume | Binance OHLC data |
| `bet_data` | avg_prob, max_prob, activity | Polymarket metrics |
| `analysis` | price_movement, actual_direction, predicted_direction, result | Correlation analysis |

### Query Examples

```bash
# View latest 5 records
echo 'scan "market_analytics", {LIMIT => 5}' | hbase shell

# Get specific symbol
echo 'scan "market_analytics", {STARTROW => "BTCUSDT", LIMIT => 10}' | hbase shell

# Count rows
echo 'count "market_analytics"' | hbase shell
```

---

## âš™ï¸ Configuration

Edit `batch_layer/spark/config.py`:

```python
# Adjust analysis thresholds
PREDICTION_THRESHOLD = 0.60      # Probability for "bullish"
MIN_PRICE_MOVEMENT = 0.001       # Min 0.1% price change

# Window settings
WINDOW_DURATION = '15 minutes'   # Aggregation window
```

---

## ðŸ“ Expected Output

### Spark Job Output

```
============================================================
Crypto Market Batch Analytics - Starting
============================================================

[1/6] Loading Binance data from Hive...
      Loaded 5760 Binance records

[2/6] Loading Polymarket data from Hive...
      Loaded 3241 Polymarket records

[3/6] Aggregating to 15-minute windows...
      Binance windows: 96
      Polymarket windows: 96

[4/6] Joining data and computing analytics...
      Generated 84 analysis records

[SAMPLE] First 5 analysis results:
+-------------------+--------+----------+-----------+--------------+---------------+------------------+
|window_start       |symbol  |window_ope|window_clos|price_movement|avg_probability|prediction_result |
+-------------------+--------+----------+-----------+--------------+---------------+------------------+
|2026-01-11 12:00:00|BTCUSDT |94532.10  |94621.45   |0.000944      |0.62           |CORRECT_BULL      |
|2026-01-11 12:15:00|BTCUSDT |94621.45  |94580.22   |-0.000436     |0.58           |UNCERTAIN         |
...

[6/6] Writing to HBase...
      Successfully wrote to HBase!

============================================================
ANALYSIS SUMMARY
============================================================
+-----------------+-----+
|prediction_result|count|
+-----------------+-----+
|CORRECT_BULL     |23   |
|FAILED_BULL      |12   |
|CORRECT_BEAR     |8    |
|UNCERTAIN        |41   |
+-----------------+-----+
```

---

## ðŸ› Troubleshooting

### No data in Hive tables

```bash
# Check if partitions exist in HDFS
hdfs dfs -ls /user/vagrant/cleansed/binance/
hdfs dfs -ls /user/vagrant/cleansed/polymarket_trade/

# Run partition repair
hive -f batch_layer/hive/repair_partitions.sql
```

### HBase connection issues

```bash
# Check HBase status
echo "status" | hbase shell

# Verify table exists
echo "list" | hbase shell

# Check Thrift server (if using happybase)
ps aux | grep HThriftServer
```

### Spark job fails

```bash
# Check logs
cat ~/Crypto-Options-vs-Rates/logs/spark_batch_analytics.log

# Run in verbose mode
spark-submit --conf spark.eventLog.enabled=false \
             --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties" \
             batch_layer/spark/batch_analytics.py
```

### Empty join results

This means Binance and Polymarket windows don't overlap. Check:
```sql
hive -e "SELECT MIN(date), MAX(date) FROM binance_klines;"
hive -e "SELECT MIN(date), MAX(date) FROM polymarket_orderbook;"
```

---

## ðŸ“š Dependencies

Installed via `requirements/python_requirements.txt`:

- `pyspark` (included in Spark distribution)
- `happybase==1.2.0` (for HBase writes)
- `pandas` (used by Spark)
- `pyarrow` (Parquet support)

Install HappyBase if missing:
```bash
pip install happybase
```

---

## ðŸ”„ Running Periodically

To run daily batch jobs:

```bash
# Add to crontab
crontab -e

# Run every day at 2 AM
0 2 * * * /home/vagrant/Crypto-Options-vs-Rates/batch_layer/run_batch_analytics.sh >> /home/vagrant/Crypto-Options-vs-Rates/logs/batch_cron.log 2>&1
```

Or run manually for specific dates:
```bash
# Process yesterday's data
batch_layer/run_batch_analytics.sh $(date -d "yesterday" +%Y-%m-%d)

# Process last 7 days
for i in {0..6}; do
    batch_layer/run_batch_analytics.sh $(date -d "$i days ago" +%Y-%m-%d)
done
```

---

## ðŸ“Š Performance Notes

- **Processing Speed**: ~1000 records/second on local mode
- **Memory**: Recommended 2GB driver + executor memory
- **Optimization**: Data is cached during window aggregation
- **Partitioning**: Hive tables partitioned by date for efficient querying

---

## ðŸŽ“ Learning Objectives Met

âœ… **Data Storage (2 pts)**: HBase with proper schema and column families  
âœ… **Batch Analysis (5 pts)**: PySpark job with windowing, joins, and analytics  
âœ… **Integration**: Reads from Hive, writes to HBase (Lambda architecture)  
âœ… **Documentation**: Complete setup and execution guide

---

## ðŸ†˜ Support

If you encounter issues:

1. Check logs in `logs/` directory
2. Verify all services are running: `sudo /home/vagrant/scripts/bootstrap.sh`
3. Ensure data exists: `hdfs dfs -ls /user/vagrant/cleansed/`
4. Test Hive connectivity: `hive -e "SHOW TABLES;"`

---

**Authors**: Batch Layer Team (Jan)  
**Last Updated**: January 2026
